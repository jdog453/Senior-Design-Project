{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57069d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os, sys\n",
    "\n",
    "sys.path.append('../utilities')\n",
    "from alignFaces import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33797934",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = {\n",
    "    \"input_dir\":\"../../facial_recognition_data/faces_dataset/andrew/\", # Path to input image\n",
    "    \"detector\":\"../../facial_detection/models/\", # Path to OpenCV's deep learning face detector\n",
    "    \"embedding_model\":\"../models/openface.nn4.small2.v1.t7\", # Path to OpenCV's embedding model\n",
    "    \"recognizer\":\"../../output/recognizer.pickle\", # Path to model trained to recognize faces\n",
    "    \"label_encoder\":\"../../output/le.pickle\", # Path to label encoder\n",
    "    \"confidence\":0.5 # Min probability to filter weak detections\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "051d4429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector...\n"
     ]
    }
   ],
   "source": [
    "# load our serialized face detector from disk\n",
    "print(\"[INFO] loading face detector...\")\n",
    "protoPath = os.path.sep.join([data_params[\"detector\"], \"deploy.prototxt\"])\n",
    "modelPath = os.path.sep.join([data_params[\"detector\"], \"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5365c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face recognizer...\n"
     ]
    }
   ],
   "source": [
    "# load our serialized face embedding model from disk\n",
    "print(\"[INFO] loading face recognizer...\")\n",
    "embedder = cv2.dnn.readNetFromTorch(data_params[\"embedding_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878656d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the actual face recognition model along with the label encoder\n",
    "recognizer = pickle.loads(open(data_params[\"recognizer\"], \"rb\").read())\n",
    "le = pickle.loads(open(data_params[\"label_encoder\"], \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "769e9a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quantifying faces...\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame0.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame100.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1000.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1020.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1040.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1060.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1080.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1100.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1120.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1140.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1160.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1180.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame120.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1200.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1220.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1240.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1260.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1280.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1300.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1320.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1340.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1360.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame1380.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame140.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame160.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame180.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame20.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame200.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame220.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame240.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame260.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame280.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame300.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame320.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame340.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame360.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame380.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame40.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame400.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame420.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame440.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame460.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame480.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame500.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame520.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame540.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame560.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame580.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame60.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame600.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame620.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame640.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame660.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame680.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame700.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame720.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame740.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame760.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame780.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame80.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame800.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame820.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame840.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame860.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame880.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame900.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame920.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame940.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame960.jpg\n",
      "../../facial_recognition_data/faces_dataset/andrew/andrew_frame980.jpg\n"
     ]
    }
   ],
   "source": [
    "# grab the paths to the input images in our dataset\n",
    "print(\"[INFO] quantifying faces...\")\n",
    "imagePaths = list(paths.list_images(data_params[\"input_dir\"]))\n",
    "print(*imagePaths, sep=\"\\n\")\n",
    "# initialize our lists of extracted facial embeddings and\n",
    "# corresponding people names\n",
    "knownEmbeddings = []\n",
    "knownNames = []\n",
    "\n",
    "# initialize the total number of faces processed\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ccba043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "andrew: 11.80%\n",
      "austen: 76.15%\n",
      "\n",
      "\n",
      "andrew: 9.63%\n",
      "austen: 71.07%\n",
      "\n",
      "\n",
      "andrew: 91.40%\n",
      "andrew: 91.40%\n",
      "\n",
      "\n",
      "andrew: 21.89%\n",
      "austen: 73.27%\n",
      "\n",
      "\n",
      "andrew: 7.09%\n",
      "austen: 86.23%\n",
      "\n",
      "\n",
      "andrew: 96.05%\n",
      "andrew: 96.05%\n",
      "\n",
      "\n",
      "andrew: 9.39%\n",
      "austen: 39.97%\n",
      "\n",
      "\n",
      "andrew: 81.95%\n",
      "andrew: 81.95%\n",
      "\n",
      "\n",
      "andrew: 30.09%\n",
      "austen: 39.75%\n",
      "\n",
      "\n",
      "andrew: 59.82%\n",
      "andrew: 59.82%\n",
      "\n",
      "\n",
      "andrew: 59.81%\n",
      "andrew: 59.81%\n",
      "\n",
      "\n",
      "andrew: 71.17%\n",
      "andrew: 71.17%\n",
      "\n",
      "\n",
      "andrew: 60.27%\n",
      "andrew: 60.27%\n",
      "\n",
      "\n",
      "andrew: 30.96%\n",
      "austen: 36.48%\n",
      "\n",
      "\n",
      "andrew: 1.54%\n",
      "austen: 83.47%\n",
      "\n",
      "\n",
      "andrew: 9.57%\n",
      "austen: 76.76%\n",
      "\n",
      "\n",
      "andrew: 57.62%\n",
      "andrew: 57.62%\n",
      "\n",
      "\n",
      "andrew: 98.19%\n",
      "andrew: 98.19%\n",
      "\n",
      "\n",
      "andrew: 76.82%\n",
      "andrew: 76.82%\n",
      "\n",
      "\n",
      "andrew: 55.21%\n",
      "andrew: 55.21%\n",
      "\n",
      "\n",
      "andrew: 61.98%\n",
      "andrew: 61.98%\n",
      "\n",
      "\n",
      "andrew: 5.36%\n",
      "austen: 91.12%\n",
      "\n",
      "\n",
      "andrew: 21.66%\n",
      "austen: 73.02%\n",
      "\n",
      "\n",
      "andrew: 95.11%\n",
      "andrew: 95.11%\n",
      "\n",
      "\n",
      "andrew: 68.14%\n",
      "andrew: 68.14%\n",
      "\n",
      "\n",
      "andrew: 75.72%\n",
      "andrew: 75.72%\n",
      "\n",
      "\n",
      "andrew: 70.67%\n",
      "andrew: 70.67%\n",
      "\n",
      "\n",
      "andrew: 12.52%\n",
      "austen: 61.77%\n",
      "\n",
      "\n",
      "andrew: 2.10%\n",
      "austen: 79.40%\n",
      "\n",
      "\n",
      "andrew: 89.86%\n",
      "andrew: 89.86%\n",
      "\n",
      "\n",
      "andrew: 61.02%\n",
      "andrew: 61.02%\n",
      "\n",
      "\n",
      "andrew: 89.43%\n",
      "andrew: 89.43%\n",
      "\n",
      "\n",
      "andrew: 8.64%\n",
      "austen: 65.70%\n",
      "\n",
      "\n",
      "andrew: 10.80%\n",
      "austen: 81.40%\n",
      "\n",
      "\n",
      "andrew: 0.60%\n",
      "austen: 78.15%\n",
      "\n",
      "\n",
      "andrew: 1.92%\n",
      "austen: 86.55%\n",
      "\n",
      "\n",
      "andrew: 62.43%\n",
      "andrew: 62.43%\n",
      "\n",
      "\n",
      "andrew: 52.60%\n",
      "andrew: 52.60%\n",
      "\n",
      "\n",
      "andrew: 63.01%\n",
      "andrew: 63.01%\n",
      "\n",
      "\n",
      "andrew: 1.11%\n",
      "fred: 60.96%\n",
      "\n",
      "\n",
      "andrew: 33.54%\n",
      "austen: 61.08%\n",
      "\n",
      "\n",
      "andrew: 22.90%\n",
      "austen: 58.77%\n",
      "\n",
      "\n",
      "andrew: 56.97%\n",
      "andrew: 56.97%\n",
      "\n",
      "\n",
      "andrew: 20.58%\n",
      "austen: 70.13%\n",
      "\n",
      "\n",
      "andrew: 0.80%\n",
      "austen: 94.22%\n",
      "\n",
      "\n",
      "andrew: 7.88%\n",
      "austen: 83.52%\n",
      "\n",
      "\n",
      "andrew: 59.58%\n",
      "andrew: 59.58%\n",
      "\n",
      "\n",
      "andrew: 60.19%\n",
      "andrew: 60.19%\n",
      "\n",
      "\n",
      "andrew: 90.68%\n",
      "andrew: 90.68%\n",
      "\n",
      "\n",
      "andrew: 29.18%\n",
      "austen: 62.31%\n",
      "\n",
      "\n",
      "andrew: 4.14%\n",
      "austen: 77.47%\n",
      "\n",
      "\n",
      "andrew: 66.59%\n",
      "andrew: 66.59%\n",
      "\n",
      "\n",
      "andrew: 67.80%\n",
      "andrew: 67.80%\n",
      "\n",
      "\n",
      "andrew: 80.07%\n",
      "andrew: 80.07%\n",
      "\n",
      "\n",
      "andrew: 22.94%\n",
      "austen: 60.27%\n",
      "\n",
      "\n",
      "andrew: 51.20%\n",
      "andrew: 51.20%\n",
      "\n",
      "\n",
      "andrew: 42.78%\n",
      "andrew: 42.78%\n",
      "\n",
      "\n",
      "andrew: 59.61%\n",
      "andrew: 59.61%\n",
      "\n",
      "\n",
      "andrew: 19.99%\n",
      "austen: 71.06%\n",
      "\n",
      "\n",
      "andrew: 3.27%\n",
      "unknown: 74.96%\n",
      "\n",
      "\n",
      "andrew: 70.22%\n",
      "andrew: 70.22%\n",
      "\n",
      "\n",
      "andrew: 42.36%\n",
      "austen: 51.15%\n",
      "\n",
      "\n",
      "andrew: 88.15%\n",
      "andrew: 88.15%\n",
      "\n",
      "\n",
      "andrew: 29.21%\n",
      "austen: 60.53%\n",
      "\n",
      "\n",
      "andrew: 69.05%\n",
      "andrew: 69.05%\n",
      "\n",
      "\n",
      "andrew: 41.19%\n",
      "andrew: 41.19%\n",
      "\n",
      "\n",
      "andrew: 61.82%\n",
      "andrew: 61.82%\n",
      "\n",
      "\n",
      "andrew: 12.10%\n",
      "austen: 82.86%\n",
      "\n",
      "\n",
      "andrew: 7.89%\n",
      "austen: 88.14%\n",
      "\n",
      "\n",
      "andrew: 26.17%\n",
      "austen: 70.27%\n",
      "\n",
      "\n",
      "andrew: 20.41%\n",
      "austen: 68.40%\n",
      "\n",
      "\n",
      "andrew: 44.52%\n",
      "austen: 45.25%\n",
      "\n",
      "\n",
      "andrew: 47.27%\n",
      "andrew: 47.27%\n",
      "\n",
      "\n",
      "andrew: 89.66%\n",
      "andrew: 89.66%\n",
      "\n",
      "\n",
      "andrew: 82.46%\n",
      "andrew: 82.46%\n",
      "\n",
      "\n",
      "andrew: 11.78%\n",
      "austen: 80.37%\n",
      "\n",
      "\n",
      "andrew: 16.97%\n",
      "austen: 80.17%\n",
      "\n",
      "\n",
      "andrew: 63.70%\n",
      "andrew: 63.70%\n",
      "\n",
      "\n",
      "andrew: 7.73%\n",
      "austen: 84.07%\n",
      "\n",
      "\n",
      "andrew: 37.85%\n",
      "andrew: 37.85%\n",
      "\n",
      "\n",
      "andrew: 88.06%\n",
      "andrew: 88.06%\n",
      "\n",
      "\n",
      "andrew: 40.26%\n",
      "andrew: 40.26%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # load the image, resize it to have a width of 600 pixels (while\n",
    "    # maintaining the aspect ratio), and then grab the image dimensions\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = imutils.resize(image, width=256)\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "    \n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the\n",
    "        # prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections\n",
    "        if confidence > data_params[\"confidence\"]:\n",
    "            # compute the (x, y)-coordinates of the bounding box for the\n",
    "            # face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # extract the face ROI\n",
    "            face = image[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "            \n",
    "            # ensure the face width and height are sufficiently large\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "            # construct a blob for the face ROI, then pass the blob\n",
    "            # through our face embedding model to obtain the 128-d\n",
    "            # quantification of the face\n",
    "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96),\n",
    "                (0, 0, 0), swapRB=True, crop=False)\n",
    "            embedder.setInput(faceBlob)\n",
    "            vec = embedder.forward()\n",
    "\n",
    "            # perform classification to recognize the face\n",
    "            preds = recognizer.predict_proba(vec)[0]    \n",
    "            j = np.argmax(preds)\n",
    "            proba = preds[j]\n",
    "            name = le.classes_[j]\n",
    "\n",
    "            # draw the bounding box of the face along with the associated\n",
    "            # probability\n",
    "            text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "            print(text)\n",
    "            print(\"\\n\")\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "790d7eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
